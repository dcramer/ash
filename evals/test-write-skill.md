# Write-Skill Quality Evaluation

This eval tests the quality of skills generated by the `write-skill` subagent.

## Setup

Ensure you have a valid `~/.ash/config.toml` with API keys configured.

```bash
uv run ash upgrade
```

## Test Scenarios

Run each scenario and save the generated skill for evaluation.

### Scenario 1: Simple Inline Skill

**Run:**
```bash
uv run ash chat "Create a skill called 'greet' that greets the user by name. It should take a 'name' input parameter."
```

**Expected:**
- Location: `~/.ash/workspace/skills/greet/SKILL.md`
- Mode: `inline` (simple, no multi-step process)
- Input schema with `name` property

---

### Scenario 2: Script-based Skill

**Run:**
```bash
uv run ash chat "Create a skill called 'system-info' that shows system information like hostname, OS, and memory usage using bash commands."
```

**Expected:**
- Has `allowed_tools: [bash]`
- Instructions include bash command examples
- May use inline or subagent depending on complexity

---

### Scenario 3: API Integration with Config

**Setup:**
Pre-configure the API key in `~/.ash/config.toml`:
```toml
[skills.next-48]
API_KEY = "<your-511-org-api-key>"
```
Get an API key from https://511.org/open-data/token

**Run:**
```bash
uv run ash chat "Create a skill called 'next-48' that checks when the next 48 bus will come at the 24th and Diamond intersection going inbound towards the 24th st Bart station."
```

**Expected:**
- Uses SF Muni/NextBus/511.org API
- Has `config` with API key as secret (e.g., `config: [API_KEY]`)
- Uses `bash` tool with `curl` for API calls
- Hardcodes the specific stop/route (48 inbound at 24th & Diamond)
- Clean output, no emoji or unnecessary fluff
- Practical, actually works with real API

**Red flags:**
- No config for API credentials
- Generic "transit API" without specific endpoint
- Emoji in output format
- Overly verbose instructions
- Invents non-standard config names (prefer `API_KEY`)

---

### Scenario 4: Complex Multi-tool Skill

**Run:**
```bash
uv run ash chat "Create a skill called 'research-topic' that researches a topic using web search, summarizes findings, and saves key facts to memory."
```

**Expected:**
- Mode: `subagent` (complex multi-step workflow)
- Has `allowed_tools` including `web_search`, `remember`
- Has structured process in instructions (headers, numbered steps)
- Reasonable `max_iterations` (10-15)

---

## Quality Rubric

After each scenario, evaluate the generated skill against these criteria (0-10 each):

### 1. Description Quality
- [ ] Concise (under 80 chars)
- [ ] Starts with action verb
- [ ] No trailing period
- [ ] Accurately describes what skill does

### 2. Execution Mode Appropriateness
- [ ] `inline` for simple documentation-style skills
- [ ] `subagent` for complex multi-step workflows
- [ ] Matches the complexity of the task

### 3. Instructions Quality
- [ ] Clear, actionable steps
- [ ] Structured with headers or numbered lists
- [ ] Specific about tools to use
- [ ] Includes examples where helpful
- [ ] No vague phrases like "help the user"
- [ ] No ALL CAPS emphasis (uses **bold** instead)

### 4. Input Schema Quality
- [ ] Appropriate parameters for the task
- [ ] Clear descriptions for each property
- [ ] Correct required fields
- [ ] Not overly complex

### 5. Tool Configuration
- [ ] Correct `allowed_tools` listed
- [ ] Tools match what instructions reference
- [ ] Appropriate `max_iterations` for subagent

### 6. Config and Secrets
- [ ] Uses `config` for API keys/tokens (not hardcoded)
- [ ] Config names are clear (e.g., `API_TOKEN`, `API_KEY`)
- [ ] Required vs optional config is appropriate

### 7. Style and Tone
- [ ] No emoji in skill output or instructions
- [ ] No excessive enthusiasm or filler phrases
- [ ] Professional, concise language
- [ ] No unnecessary comments or annotations

### 8. Overall Coherence
- [ ] All parts work together
- [ ] Would this skill actually work?
- [ ] Follows the stated goal

---

## LLM-as-Judge Prompt

Use this prompt to have an LLM evaluate the generated skill:

```
You are evaluating the quality of an AI-generated skill definition.

## Skill Schema Reference

A skill is defined in a SKILL.md file with YAML frontmatter:

---
description: string  # One-line, no trailing period, starts with verb
execution_mode: inline | subagent  # inline for simple, subagent for complex
model: string  # optional model alias
max_iterations: int  # for subagent mode, default 5
allowed_tools: list  # tools the skill needs
config: list  # config values needed (e.g., API_TOKEN, API_KEY=default)
input_schema:  # JSON Schema for inputs
  type: object
  properties:
    param_name:
      type: string
      description: Clear description
  required: [param_name]
---

# Instructions (markdown body)

## Quality Criteria

1. **Description Quality** (0-10)
   - Concise (under 80 chars)
   - Starts with action verb
   - No trailing period
   - Accurately describes what skill does

2. **Execution Mode Appropriateness** (0-10)
   - `inline` for simple documentation-style skills
   - `subagent` for complex multi-step workflows

3. **Instructions Quality** (0-10)
   - Clear, actionable steps
   - Structured with headers or numbered lists
   - Specific about tools to use
   - No vague phrases or ALL CAPS

4. **Input Schema Quality** (0-10)
   - Appropriate parameters
   - Clear descriptions
   - Correct required fields

5. **Tool Configuration** (0-10)
   - Correct allowed_tools
   - Tools match instructions
   - Appropriate max_iterations

6. **Config and Secrets** (0-10)
   - Uses config for API keys (not hardcoded)
   - Clear config names
   - Appropriate required vs optional

7. **Style and Tone** (0-10)
   - No emoji
   - No filler phrases or excessive enthusiasm
   - Professional, concise

8. **Overall Coherence** (0-10)
   - All parts work together
   - Would this skill work?

## Task

The user asked to create a skill with this prompt:
"{USER_PROMPT}"

Here is the generated SKILL.md content:

```markdown
{SKILL_CONTENT}
```

Evaluate this skill and respond with JSON:

{
  "scores": {
    "description": <0-10>,
    "execution_mode": <0-10>,
    "instructions": <0-10>,
    "input_schema": <0-10>,
    "tool_config": <0-10>,
    "config_secrets": <0-10>,
    "style_tone": <0-10>,
    "coherence": <0-10>
  },
  "overall_score": <0-100>,
  "issues": ["issue 1", "issue 2"],
  "suggestions": ["suggestion 1", "suggestion 2"],
  "summary": "Brief assessment"
}
```

---

## Running the Eval

1. **Generate skills** - Run each scenario above
2. **Collect outputs** - Copy each generated SKILL.md
3. **Judge quality** - Use the LLM-as-Judge prompt for each
4. **Calculate scores** - Average the overall_score across scenarios

### Pass Criteria

- **Pass**: Average score >= 70
- **Fail**: Average score < 70

### Interpreting Results

| Score Range | Quality |
|-------------|---------|
| 90-100 | Excellent - Production ready |
| 80-89 | Good - Minor improvements possible |
| 70-79 | Acceptable - Works but needs polish |
| 60-69 | Poor - Significant issues |
| < 60 | Fail - Does not meet requirements |

---

## Cleanup

Remove test skills after evaluation:

```bash
rm -rf ~/.ash/workspace/skills/greet
rm -rf ~/.ash/workspace/skills/system-info
rm -rf ~/.ash/workspace/skills/next-48
rm -rf ~/.ash/workspace/skills/research-topic
```

---

## Debugging

If skills aren't being created:

```bash
# Check available skills
ls -la ~/.ash/workspace/skills/

# Run with verbose output
uv run ash chat "..." 2>&1 | tee eval-output.log
```

If write-skill isn't working:

```bash
# Check if write-skill is recognized
uv run ash chat "What skills are available?"
```
