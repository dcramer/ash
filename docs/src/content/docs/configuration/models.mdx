---
title: Models
description: Configure LLM models and providers
sidebar:
  order: 2
---

import { Aside } from '@astrojs/starlight/components';

Define named model configurations that can be referenced throughout Ash.

## Recommended Setup

Use Haiku for simple tasks (fast, cheap) and Sonnet for complex tasks:

```toml
[models.default]
provider = "anthropic"
model = "claude-haiku-4-5-20251001"
temperature = 0.7
max_tokens = 4096

[models.sonnet]
provider = "anthropic"
model = "claude-sonnet-4-5-20250929"
max_tokens = 8192

# Override model for specific skills
[skills.debug]
model = "sonnet"

[skills.code-review]
model = "sonnet"

[skills.research]
model = "sonnet"
```

<Aside type="tip">
  This setup saves money by using Haiku for conversation while using Sonnet for complex skills that need more reasoning power.
</Aside>

## Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `provider` | string | required | `"anthropic"` or `"openai"` |
| `model` | string | required | Model identifier |
| `temperature` | float | `null` | Sampling temperature (0.0-1.0) |
| `max_tokens` | int | `4096` | Maximum response tokens |

<Aside type="tip">
  Set `temperature` to `null` (or omit it) for reasoning models that don't support temperature control.
</Aside>

## Per-Skill Model Override

Override the model used by specific skills:

```toml
[skills.debug]
model = "sonnet"  # Use sonnet for debugging

[skills.code-review]
model = "sonnet"  # Use sonnet for code review
```

Model resolution order:
1. `[skills.<name>] model` in config
2. `model` in skill definition (SKILL.md)
3. `"default"` fallback

## Multiple Models

Define multiple named models:

```toml
[models.default]
provider = "anthropic"
model = "claude-haiku-4-5-20251001"

[models.sonnet]
provider = "anthropic"
model = "claude-sonnet-4-5-20250929"

[models.reasoning]
provider = "anthropic"
model = "claude-opus-4-5-20251101"
# No temperature for reasoning models
```

Use models by alias:

```bash
uv run ash chat --model sonnet "Complex question"
uv run ash chat --model reasoning "Very complex problem"
```

## Provider API Keys

Set API keys at the provider level:

```toml
[anthropic]
api_key = "sk-ant-..."

[openai]
api_key = "sk-..."
```

Or use environment variables:

```bash
export ANTHROPIC_API_KEY=sk-ant-...
export OPENAI_API_KEY=sk-...
```

## API Key Resolution

Keys are resolved in this order:

1. Provider config (`[anthropic].api_key`)
2. Environment variable (`ANTHROPIC_API_KEY`)

## Supported Models

### Anthropic

- `claude-haiku-4-5-20251001` (recommended for default - fast, cheap)
- `claude-sonnet-4-5-20250929` (recommended for complex tasks)
- `claude-opus-4-5-20251101` (reasoning tasks)

### OpenAI

- `gpt-5-mini` (recommended for default - fast, cheap)
- `gpt-5` (more capable)

## Default Model Requirement

A model named `default` is required:

```toml
[models.default]
provider = "anthropic"
model = "claude-haiku-4-5-20251001"
```

This model is used when no `--model` flag is specified.
