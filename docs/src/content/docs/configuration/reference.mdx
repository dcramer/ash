---
title: Configuration Reference
description: Reference for config.toml options
sidebar:
  order: 2
---

## Globals

```toml
workspace = "~/.ash/workspace"
timezone = "America/Los_Angeles"
```

| Key | Type | Default |
|-----|------|---------|
| `workspace` | path | `~/.ash/workspace` |
| `timezone` | string (IANA TZ) | system timezone |

## Models (`[models.<alias>]`)

```toml
[models.default]
provider = "openai-oauth"
model = "gpt-5.2"
temperature = 0.7
max_tokens = 4096
reasoning = "medium"
```

| Key | Type | Notes |
|-----|------|-------|
| `provider` | `anthropic` \| `openai` \| `openai-oauth` | Required |
| `model` | string | Required |
| `temperature` | float \| null | Optional |
| `max_tokens` | int | Default `4096` |
| `thinking` | `off|minimal|low|medium|high` \| null | Anthropic-specific |
| `reasoning` | `low|medium|high` \| null | OpenAI-specific |

## Provider Keys

```toml
[openai]
api_key = "sk-..."

[anthropic]
api_key = "sk-ant-..."
```

| Section | Key | Env Fallback |
|---------|-----|--------------|
| `[openai]` | `api_key` | `OPENAI_API_KEY` |
| `[anthropic]` | `api_key` | `ANTHROPIC_API_KEY` |

## Sandbox (`[sandbox]`)

```toml
[sandbox]
image = "ash-sandbox:latest"
timeout = 60
memory_limit = "512m"
cpu_limit = 1.0
runtime = "runc"
network_mode = "bridge"
dns_servers = []
http_proxy = ""
workspace_access = "rw"
sessions_access = "ro"
chats_access = "ro"
source_access = "none"
mount_prefix = "/ash"
apt_packages = []
python_packages = []
setup_command = ""
```

| Key | Type | Default |
|-----|------|---------|
| `image` | string | `ash-sandbox:latest` |
| `timeout` | int | `60` |
| `memory_limit` | string | `512m` |
| `cpu_limit` | float | `1.0` |
| `runtime` | `runc` \| `runsc` | `runc` |
| `network_mode` | `none` \| `bridge` | `bridge` |
| `dns_servers` | list[string] | `[]` |
| `http_proxy` | string \| null | `null` |
| `workspace_access` | `none` \| `ro` \| `rw` | `rw` |
| `sessions_access` | `none` \| `ro` | `ro` |
| `chats_access` | `none` \| `ro` | `ro` |
| `source_access` | `none` \| `ro` | `none` |
| `mount_prefix` | string | `/ash` |
| `apt_packages` | list[string] | `[]` |
| `python_packages` | list[string] | `[]` |
| `setup_command` | string \| null | `null` |

## Memory (`[memory]`)

```toml
[memory]
max_context_messages = 20
context_token_budget = 100000
recency_window = 10
system_prompt_buffer = 8000
compaction_enabled = true
auto_gc = true
max_entries = null
extraction_enabled = true
extraction_model = null
extraction_min_message_length = 20
extraction_debounce_seconds = 30
extraction_confidence_threshold = 0.7
extraction_context_messages = 8
extraction_verification_enabled = true
extraction_verification_model = null
```

| Key | Type | Default |
|-----|------|---------|
| `max_context_messages` | int | `20` |
| `context_token_budget` | int | `100000` |
| `recency_window` | int | `10` |
| `system_prompt_buffer` | int | `8000` |
| `compaction_enabled` | bool | `true` |
| `auto_gc` | bool | `true` |
| `max_entries` | int \| null | `null` |
| `extraction_enabled` | bool | `true` |
| `extraction_model` | string \| null | `null` |
| `extraction_min_message_length` | int | `20` |
| `extraction_debounce_seconds` | int | `30` |
| `extraction_confidence_threshold` | float | `0.7` |
| `extraction_context_messages` | int | `8` |
| `extraction_verification_enabled` | bool | `true` |
| `extraction_verification_model` | string \| null | `null` |

## Conversation (`[conversation]`)

```toml
[conversation]
recency_window = 10
gap_threshold_minutes = 15
reply_context_window = 3
chat_history_limit = 5
```

## Sessions (`[sessions]`)

```toml
[sessions]
mode = "persistent"
max_concurrent = 2
```

## Embeddings (`[embeddings]`)

```toml
[embeddings]
provider = "openai"
model = "text-embedding-3-small"
```

## Skills

```toml
[skills]
auto_sync = false
update_interval = 24

[[skills.sources]]
repo = "owner/repo"

[skills.research]
enabled = true
model = "mini"
PERPLEXITY_API_KEY = "..."
```

## Agents (`[agents.<name>]`)

```toml
[agents.research]
model = "codex"
max_iterations = 50
```

## Other Optional Sections

```toml
[env]
FOO = "bar"
HOME_DIR = "$HOME"

[telegram]
bot_token = "123:abc"
allowed_users = ["@me"]
allowed_groups = []
group_mode = "mention"

[telegram.passive]
enabled = false

[brave_search]
api_key = "..."

[sentry]
dsn = "https://..."

[server]
host = "127.0.0.1"
port = 8080
webhook_path = "/webhook"
```
