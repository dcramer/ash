---
title: LLM Providers
description: Configure model aliases and provider credentials
sidebar:
  order: 8
---

import { Aside } from '@astrojs/starlight/components';

Ash uses named model aliases under `[models.*]`. You pick a default alias, then optionally route specific workflows to cheaper or stronger models.

## LLM Providers In 30 Seconds

- Configure one required alias: `[models.default]`
- Point each alias at a provider model (`openai` or `anthropic`)
- Set provider API keys in config or environment variables
- Use alias names in CLI (`--model`) and skill overrides

## Quick Start

Use this as a practical baseline:

```toml
# Required default model
[models.default]
provider = "openai"
model = "gpt-5.2"
max_tokens = 4096

# Fast/cheap option for lightweight tasks
[models.mini]
provider = "openai"
model = "gpt-5-mini"
max_tokens = 4096

# Coding-focused option
[models.codex]
provider = "openai"
model = "gpt-5.2-codex"
max_tokens = 8192

# Provider credentials (or use env vars)
[openai]
api_key = "sk-..."
```

Test alias selection:

```bash
uv run ash chat --model mini "Summarize this changelog"
uv run ash chat --model codex "Refactor this Python function"
```

## Configure Providers And API Keys

Provider credentials can come from config:

```toml
[openai]
api_key = "sk-..."

[anthropic]
api_key = "sk-ant-..."
```

Or environment variables:

```bash
export OPENAI_API_KEY=sk-...
export ANTHROPIC_API_KEY=sk-ant-...
```

Resolution order:

1. Config (`[openai].api_key`, `[anthropic].api_key`)
2. Environment (`OPENAI_API_KEY`, `ANTHROPIC_API_KEY`)

## Model Alias Options

Each alias block supports:

```toml
[models.default]
provider = "openai"      # Required: "openai" or "anthropic"
model = "gpt-5.2"        # Required: provider model id
temperature = 0.7         # Optional: omit/null for reasoning-first models
max_tokens = 4096         # Optional: default is 4096
reasoning = "high"       # Optional: OpenAI reasoning effort (low|medium|high)
thinking = "medium"      # Optional: Anthropic thinking budget
```

<Aside type="note">
  `reasoning` applies to OpenAI reasoning models. `thinking` applies to Claude models that support extended thinking.
</Aside>

## Tune Reasoning Only When Needed

For OpenAI reasoning effort:

```toml
[models.pro]
provider = "openai"
model = "gpt-5.2-pro"
reasoning = "high"  # low | medium | high
```

For Claude extended thinking:

```toml
[models.reasoning]
provider = "anthropic"
model = "claude-opus-4-6"
thinking = "medium"  # off | minimal | low | medium | high
```

Start with defaults. Increase reasoning/thinking only for tasks that need deeper analysis.

## Per-Skill Model Overrides

Skills can target model aliases directly:

```toml
[skills.debug]
model = "codex"        # Use coding-focused alias

[skills.research]
model = "default"      # Use standard alias
```

Skill model resolution order:

1. `[skills.<name>].model` in config
2. `model` in `SKILL.md`
3. `default` alias

## Troubleshooting

### Unknown model alias

```bash
# Check aliases and spelling
uv run ash config show

# Retry with a known alias
uv run ash chat --model default "hello"
```

### Provider auth failures

```bash
# Validate config shape and required fields
uv run ash config validate

# Diagnose environment and integration issues
uv run ash doctor
```

Common fix: ensure the provider key is present in config or exported env var.

### Responses are cut off

Increase `max_tokens` for the alias used by that workflow.

```toml
[models.default]
provider = "openai"
model = "gpt-5.2"
max_tokens = 8192
```

## Reference (Advanced)

Key implementation files:

- `src/ash/llm/base.py` - provider interface
- `src/ash/llm/openai.py` - OpenAI implementation
- `src/ash/llm/anthropic.py` - Anthropic implementation
- `src/ash/llm/registry.py` - provider registration and lookup
- `src/ash/llm/types.py` - message/tool-call data types

Tool definitions are passed to the model from the tool registry. Providers return assistant messages and tool-call requests in normalized Ash types.
